---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r, warning=FALSE, message=FALSE}
library(here)
library(skimr)
library(randomForest)
library(OptimalCutpoints)
library(Epi)
library(pROC)

i_am("research/lets-make-a-model.Rmd")

source(paste0(here(),"/R/get-data.R"))
source(paste0(here(),"/R/modelling-functions.R"))

footy_tipper_df <- get_data(year_span = 2012:2023)

skim(footy_tipper_df)
```

```{r, warning=FALSE, message=FALSE}
# define the feature set
footy_tipper_df <- footy_tipper_df %>%
  filter(game_state_name == 'Final',
         !is.na(home_team_result)) %>%
  select(-c(start_time, start_time_utc, game_state_name, crowd, city, broadcast_channel1, broadcast_channel2, broadcast_channel3, team_final_score_away, team_final_score_home, game_id, round_id))

# Fit a model
rf_model <- randomForest(factor(home_team_result) ~ .,
                   data = footy_tipper_df,
                   ntree = 500,
                   importance = T)

rf_model
```

```{r, warning=FALSE, message=FALSE}
# Set up clusters - this speeds up cv training (4 clusters ~ twice as fast)
cl <- makePSOCKcluster(detectCores())
registerDoParallel(cl)

# Set up caret CV options
ctrl <- trainControl(method = "repeatedcv",
                     number = 5,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     sampling = NULL,
                     verboseIter = TRUE)
set.seed(69)


# The new big loop
cv1 <- train(x = footy_tipper_df %>% select(-home_team_result) %>% as.data.frame,
             y = footy_tipper_df$home_team_result,
             method = "rf",
             metric = "ROC",
             trControl = ctrl,
             tuneGrid = expand.grid(mtry = 1:((ncol(footy_tipper_df) - 1))))

# When you are done:
stopCluster(cl)

# print results
cv1

# final model diagnostics
rf_model <- cv1$finalModel
rf_model
```

```{r, warning=FALSE, message=FALSE}
# compare the out-of-bag predictions with the seen outcomes
prob <- rf_model$votes %>% as.data.frame %>% mutate(response = rf_model$y)

# determine an optimal cut off parameter
cut <- optimal.cutpoints(X = "Win", status = "response",
                         tag.healthy = "Loss", methods = "MaxKappa",
                         data = prob)

# assign this parameter back onto the rf_model object
cutoff <- cut$MaxKappa$Global$optimal.cutoff$cutoff
cutoff <- c(cutoff, 1 - cutoff)
rf_model$forest$cutoff <- cutoff

cut
```

```{r, warning=FALSE, message=FALSE}
# print roc and auc measures
roc(footy_tipper_df$home_team_result, rf_model$votes[,"Win"])
roc(footy_tipper_df$home_team_result, rf_model$votes[,"Win"]) %>% plot(main = 'Model ROC curve')
```

```{r, warning=FALSE, message=FALSE}
# new vals
footy_tipper_df <- footy_tipper_df %>%
  mutate(prediction = as.factor(ifelse(rf_model$votes[,"Win"] > cutoff[1], "Win", "Loss")))

# print confusion matrix statistics
new_cm <- footy_tipper_df %>% 
  with(table(factor(footy_tipper_df$home_team_result), factor(footy_tipper_df$prediction)))

confusionMatrix(new_cm, positive = "Win")
```

```{r, warning=FALSE, message=FALSE}
importance(rf_model) %>% t() %>% .[1,] %>% 
  as.tibble(rownames = "variable") %>%
  ggplot(aes(x = variable, y = value, fill = variable)) +
  geom_bar(stat = "identity") +
    coord_flip() +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Importance of variables",
       x = NULL, y = "Importance")
```

